---
title: "Classic Data Analytics I - W2"
subtitle: "Big Data ç»å…¸æ•°æ®åˆ†æ1 - W2"
layout: post
author: "Aaron"
header-style: text
hidden: true
tags:
  - Big Data
  - Machine Learning
  - ç¬”è®°
---

## Text Analytics

### Text Data and Applications

Text Analyticsï¼š

- Clustering(èšç±»)
  - Grouping documents based on their hidden topics
- Classification(åˆ†ç±»)
  - Spam email detection based on email content\\
- Sentiment analysis
  - It extracts social sentiment from a document (positive, negative, neutral)

Characteristics of Text Data

- Different types: characters, numbers, punctuations
- High-fequency words: a, the, in. To, is
- Different  forms of words

### Feature Extraction (ç‰¹å¾æå–)

**Feature extraction on text data**: The process of transforming raw data into numerical features that can be processed while preserving the information in the orginal data set

**After Feature Extraction**: feed the extracted features into data analytical methods

**Tokenization**(æ ‡è®°åŒ–/è¯æ±‡åˆ‡åˆ†): 1, convert the text into a sequence of tokens(words/terms) 2, Observation meaningless or meaningful

![image-20220207112726279](https://raw.githubusercontent.com/Dr-Aaron/forpic/master/uPic/image-20220207112726279.png)

- Simple approach: 1,split by spaces. 2, ignore all numbers and punctuation 3, use case-insensitive strings as tokensï¼ˆç©ºæ ¼åˆ†éš”ã€ä¸åŒºåˆ†æ ‡ç‚¹ã€ä¸åŒºåˆ†å¤§å°å†™ï¼‰
- **Stopwords**: function words: a, the, in, to. Pronouns  I, he, she, it
- **Stemming**: For matching purpose, convert keywords in the documents to their stems(base word forms)(å°†è¯æ±‡è½¬æ¢ä¸ºåŸºæœ¬è¯å¹²)
  - **Poter Stemmer** : procedure for removing known prefixes/suffixes(åˆ é™¤å•è¯å‰åç¼€)
  - For example computer, computational, computation... ->comput
  - Side effects: may produce stems that are not words, or different meaning from the original word;(organization ->organ)
  - ï¼ˆè¡¥å……ï¼šEnglish: NLTK, SpaCy, Stanford...ä¸­æ–‡ï¼šTHULAC, FoolNLTK, HanLP, Ictclas, HIT...)

Text Corpus and Vocabulary

- Text corpus: the set of texts used for the task(è¯¥ä»»åŠ¡çš„è¯­æ–™åº“), The set of unique words is referred to as the vocabulary.

**Bag of Words Model**

Convert each document to a bag(multiset) of words/terms. A bag allows multiple occurrence of a term.

Bags(Multiset) of words with Term Frequence.

<img src="https://raw.githubusercontent.com/Dr-Aaron/forpic/master/uPic/image-20220209202707642.png" alt="image-20220209202707642" style="zoom:50%;" />

Bag-of-words Model is one Example of Vector Space Model

- After feature extraction,We convert each document to a vector, convert the text corpus to a matrix.(æ–‡æ¡£è½¬æ¢ä¸ºå‘é‡ï¼Œè¯­æ–™åº“è½¬æ¢ä¸ºçŸ©é˜µ)

Vector space Model

- The vector space model is defined by basis vectors.(å‘é‡ç©ºé—´ç”±åŸºå‘é‡æ‰€å®šä¹‰)
  - Each term in vocabulary defines a basis vector $T_i$.(æ¯ä¸ªæœ¯è¯­å®šä¹‰äº†ä¸€ä¸ªåŸºå‘é‡)
  - Each basis vector is orthogonal to each other.(æ¯ä¸ªåŸºå‘é‡å½¼æ­¤æ­£äº¤)

- Document $D_j$ As T-dimensional vector
  - t is the size of vocabulary.(tæ˜¯é¢„å¤„ç†åä¿ç•™çš„æœ¯è¯­çš„æ•°é‡)
  - $D_j$  = ($w_1j$ , $w_2j$ , $w_3j$ ,  ... $w_tj$ , )
  - $w_ij$  denotes the weight of term $T_i$ in a document $D_j$. 

<img src="https://raw.githubusercontent.com/Dr-Aaron/forpic/master/uPic/image-20220209213500523.png" alt="image-20220209213500523" style="zoom:50%;" />

<img src="https://raw.githubusercontent.com/Dr-Aaron/forpic/master/uPic/image-20220209213533055.png" alt="image-20220209213533055" style="zoom:50%;" />

### Similarity Search

#### Cosine Similarity

With respect to query text: {cheap, quiet, nice, hotel}

1. Convert query text to query vector [1, 1, 1, 0, 1]

<img src="https://raw.githubusercontent.com/Dr-Aaron/forpic/master/uPic/image-20220209214129073.png" alt="image-20220209214129073" style="zoom:50%;" />

2. We need a similarity measure between query and documents

   [1, 1, 1, 0, 1]

   <img src="https://raw.githubusercontent.com/Dr-Aaron/forpic/master/uPic/image-20220209214228380.png" alt="image-20220209214228380" style="zoom:50%;" />

Inner Product: ğª â‹… $ğ± = ğ‘¥_1ğ‘_1 + â‹¯ + ğ‘¥_ğ‘›ğ‘_ğ‘› $ 

The magnitude/length of a n-dimensional vector $x = [x_1, x_2,...,x_n]$
$$
||x||=\sqrt[]{x_1^2+...x_n^2}
$$
Cosine similarity measures the cosine of the angle between vectors(ä½™å¼¦ç›¸ä¼¼åº¦ï¼šå‘é‡ä¹‹é—´çš„å¤¹è§’ä½™å¼¦)
$$
CosSim(q,x)=q\cdot x \over ||q||\ |x||
$$
Inner Product and Cosine Similarity

- Both are defined in the inner product space

- Cosine similarity only cares about angle difference(ä½™å¼¦ç›¸ä¼¼åº¦åªå…³å¿ƒè§’åº¦)

- Inner product cares about angle and magnitude(å†…ç§¯å…³æ³¨è§’åº¦å’Œå¤§å°)

#### TF-IDF

TF-IDF: Determine the importance of a Word, Term Frequency.

1. More frequent terms in a document are more important. $tf_{ij} = frequency\  of\  term\ i\ in \ document \ j$

2. Inverse ducument frequency IDF

   Terms that appear in many different documents are less indicative of overall topic in a document.

   $ğ‘‘ğ‘“_ğ‘–$ = number of documents containing term ğ‘– 

   $ğ‘–ğ‘‘ğ‘“_ğ‘–$ = inverse document frequency of term ğ‘–

   â€‹        = $log_2(\cfrac N{df_i})$

â€‹	TF-IDF weighting: The combined term importance indicator is called tf-idf weighting:  $w_{ij}=tf_{ij}\cdot idf_i$

- A term has high weight when: **it occurs frequently in the document, but rarely in the rest of the collection.**

==Exercise slide53==

<img src="https://raw.githubusercontent.com/Dr-Aaron/forpic/master/uPic/image-20220210143256766.png" alt="image-20220210143256766" style="zoom: 50%;" />

<img src="https://raw.githubusercontent.com/Dr-Aaron/forpic/master/uPic/image-20220210143328410.png" alt="image-20220210143328410" style="zoom: 50%;" />

## Unsupervised Algorithms

### Clustering

#### Applications and Concepts

Clustering is the task of grouping a set of objects in such a way that objects in the same group are more similar(**similarity measures**ç›¸ä¼¼åº¦åº¦é‡) to each other than to those in other groups(clusters).å°†å¯¹è±¡è¿›è¡Œåˆ†ç»„ï¼Œä½¿åŒä¸€ç»„å¯¹è±¡ä¸­çš„ç›¸ä¼¼ç¨‹åº¦æ¯”å…¶ä»–ç»„æ›´é«˜

Clustering is used:

- As a stand-tool to get insight inito data distribution.(é›†ç¾¤å¯è§†åŒ–)
- As a preprocessing step for other algorithms.(æ•°æ®æ¸…æ´—å’Œå‹ç¼©)

**Outlier** Analysis by clustering

- Outliers are objects that do not belong to any cluster or form clusters of very small cardinality
- Distances on numerical values

<img src="https://raw.githubusercontent.com/Dr-Aaron/forpic/master/uPic/image-20220210151219703.png" alt="image-20220210151219703" style="zoom: 50%;" />

<img src="https://raw.githubusercontent.com/Dr-Aaron/forpic/master/uPic/image-20220210151605931.png" alt="image-20220210151605931" style="zoom:50%;" />

Euclidean distance:æ¬§æ°è·
$$
d(x_i, x_j)= \sqrt[2]{|x_{i1}-x_{j1}|^2+|x_{i2}-x_{j2}|+...+|x_{ip}-x_{jp}|^2}
$$

- $d(x_i, x_j)\ge0$  (non-negativity)
- $d(x_i, x_j)=0$ (coincidence) å¥‘åˆ
- $d(x_i, x_j)=d(x_j, x_i)$ (symmetry) å¯¹ç§°
- $d(x_i, x_j)\le d(x_i, x_k)+d(x_k, x_j)$ (triangular inequality)ä¸‰è§’ä¸ç­‰å¼

Also one can use **weighted** distance
$$
d(x_i, x_j)= \sqrt[2]{w_1|x_{i1}-x_{j1}|^2+w_2|x_{i2}-x_{j2}|+...+w_p|x_{ip}-x_{jp}|^2}
$$
The centroid or geometric center of a plane figure is the arthmetic mean position of all the points in the shape.å¹³é¢å›¾å½¢çš„è´¨å¿ƒ/å‡ ä½•ä¸­å¿ƒæ˜¯è¯¥å½¢çŠ¶æ‰€æœ‰ç‚¹åæ ‡ç®—æœ¯å¹³å‡å€¼ä½ç½®

![image-20220210153456475](https://raw.githubusercontent.com/Dr-Aaron/forpic/master/uPic/image-20220210153456475.png)

#### K-Means

Partitioning methodåˆ’åˆ†æ–¹æ³•ï¼šConstruct a partition of a database D of objects into a set of k clusters.å°†æ•°æ®åº“Dä¸­çš„Nä¸ªå¯¹è±¡åˆ’åˆ†ä¸ºkä¸ªé›†ç¾¤

- Each cluster is represented by the center of the cluster.

1. k initial **random centroids** in the data domain. åˆå§‹æ•°æ®åŸŸä¸­çš„éšæœºè´¨å¿ƒ
2. Assign objects to nearest centroid to from clusters.
3. Update centroids by conputing the mean ofa each cluster.é€šè¿‡è®¡ç®—æ¯ä¸ªç°‡çš„å¹³å‡å€¼è®¡ç®—è´¨å¿ƒ
4. go to step 2

<img src="https://raw.githubusercontent.com/Dr-Aaron/forpic/master/uPic/image-20220210155010107.png" alt="image-20220210155010107" style="zoom:67%;" />

Pseudo-code

<img src="https://raw.githubusercontent.com/Dr-Aaron/forpic/master/uPic/image-20220210155208934.png" alt="image-20220210155208934" style="zoom:50%;" />

**Strength**: Relatively efficient: O(tkn), wheren is # objects, k is $cluster, and t is #iteration.(å¯¹è±¡æ•°é‡ï¼Œç°‡æ•°é‡ï¼Œ è¿­ä»£æ¬¡æ•°) Normallymk,t<<n

**Weakness**: 

- Applicable only when *mean* is defined (what about categorical data)?ä»…åœ¨å¹³å‡æ•°æ®æœ‰æ•ˆæ—¶æœ‰ç”¨

- Need to specify *k,* the *number* of clusters, in advanceéœ€è¦åˆ¶å®šç°‡çš„æ•°é‡

- Unable to handle noisy data and outliers.æ— æ³•å¤„ç†å™ªå£°å’Œå¼‚å¸¸å€¼

- Not suitable to discover clusters with non-convex shapes.ä¸èƒ½åˆ¤æ–­éå‡¸ç°‡

Distance between clusters

- Single Link: smallest distance between any points in two clusters
- Complete Link: largest distance between any points in two clusters
- Centroid: distance between the centroids of two clusters
- Average Link: average distance of all pairwise points in clusters
- Average of the distances of the 4*3 pairs of points in the example 





