I"U*<h2 id="text-analytics">Text Analytics</h2>

<h3 id="text-data-and-applications">Text Data and Applications</h3>

<p>Text Analyticsï¼š</p>

<ul>
  <li>Clustering(èšç±»)
    <ul>
      <li>Grouping documents based on their hidden topics</li>
    </ul>
  </li>
  <li>Classification(åˆ†ç±»)
    <ul>
      <li>Spam email detection based on email content\</li>
    </ul>
  </li>
  <li>Sentiment analysis
    <ul>
      <li>It extracts social sentiment from a document (positive, negative, neutral)</li>
    </ul>
  </li>
</ul>

<p>Characteristics of Text Data</p>

<ul>
  <li>Different types: characters, numbers, punctuations</li>
  <li>High-fequency words: a, the, in. To, is</li>
  <li>Different  forms of words</li>
</ul>

<h3 id="feature-extraction-ç‰¹å¾æå–">Feature Extraction (ç‰¹å¾æå–)</h3>

<p><strong>Feature extraction on text data</strong>: The process of transforming raw data into numerical features that can be processed while preserving the information in the orginal data set</p>

<p><strong>After Feature Extraction</strong>: feed the extracted features into data analytical methods</p>

<p><strong>Tokenization</strong>(æ ‡è®°åŒ–/è¯æ±‡åˆ‡åˆ†): 1, convert the text into a sequence of tokens(words/terms) 2, Observation meaningless or meaningful</p>

<p><img src="https://raw.githubusercontent.com/Dr-Aaron/forpic/master/uPic/image-20220207112726279.png" alt="image-20220207112726279" /></p>

<ul>
  <li>Simple approach: 1,split by spaces. 2, ignore all numbers and punctuation 3, use case-insensitive strings as tokensï¼ˆç©ºæ ¼åˆ†éš”ã€ä¸åŒºåˆ†æ ‡ç‚¹ã€ä¸åŒºåˆ†å¤§å°å†™ï¼‰</li>
  <li><strong>Stopwords</strong>: function words: a, the, in, to. Pronouns  I, he, she, it</li>
  <li><strong>Stemming</strong>: For matching purpose, convert keywords in the documents to their stems(base word forms)(å°†è¯æ±‡è½¬æ¢ä¸ºåŸºæœ¬è¯å¹²)
    <ul>
      <li><strong>Poter Stemmer</strong> : procedure for removing known prefixes/suffixes(åˆ é™¤å•è¯å‰åç¼€)</li>
      <li>For example computer, computational, computationâ€¦ -&gt;comput</li>
      <li>Side effects: may produce stems that are not words, or different meaning from the original word;(organization -&gt;organ)</li>
      <li>ï¼ˆè¡¥å……ï¼šEnglish: NLTK, SpaCy, Stanfordâ€¦ä¸­æ–‡ï¼šTHULAC, FoolNLTK, HanLP, Ictclas, HITâ€¦)</li>
    </ul>
  </li>
</ul>

<p>Text Corpus and Vocabulary</p>

<ul>
  <li>Text corpus: the set of texts used for the task(è¯¥ä»»åŠ¡çš„è¯­æ–™åº“), The set of unique words is referred to as the vocabulary.</li>
</ul>

<p><strong>Bag of Words Model</strong></p>

<p>Convert each document to a bag(multiset) of words/terms. A bag allows multiple occurrence of a term.</p>

<p>Bags(Multiset) of words with Term Frequence.</p>

<p><img src="https://raw.githubusercontent.com/Dr-Aaron/forpic/master/uPic/image-20220209202707642.png" alt="image-20220209202707642" style="zoom:50%;" /></p>

<p>Bag-of-words Model is one Example of Vector Space Model</p>

<ul>
  <li>After feature extraction,We convert each document to a vector, convert the text corpus to a matrix.(æ–‡æ¡£è½¬æ¢ä¸ºå‘é‡ï¼Œè¯­æ–™åº“è½¬æ¢ä¸ºçŸ©é˜µ)</li>
</ul>

<p>Vector space Model</p>

<ul>
  <li>The vector space model is defined by basis vectors.(å‘é‡ç©ºé—´ç”±åŸºå‘é‡æ‰€å®šä¹‰)
    <ul>
      <li>Each term in vocabulary defines a basis vector $T_i$.(æ¯ä¸ªæœ¯è¯­å®šä¹‰äº†ä¸€ä¸ªåŸºå‘é‡)</li>
      <li>Each basis vector is orthogonal to each other.(æ¯ä¸ªåŸºå‘é‡å½¼æ­¤æ­£äº¤)</li>
    </ul>
  </li>
  <li>Document $D_j$ As T-dimensional vector
    <ul>
      <li>t is the size of vocabulary.(tæ˜¯é¢„å¤„ç†åä¿ç•™çš„æœ¯è¯­çš„æ•°é‡)</li>
      <li>$D_j$  = ($w_1j$ , $w_2j$ , $w_3j$ ,  â€¦ $w_tj$ , )</li>
      <li>$w_ij$  denotes the weight of term $T_i$ in a document $D_j$.</li>
    </ul>
  </li>
</ul>

<p><img src="https://raw.githubusercontent.com/Dr-Aaron/forpic/master/uPic/image-20220209213500523.png" alt="image-20220209213500523" style="zoom:50%;" /></p>

<p><img src="https://raw.githubusercontent.com/Dr-Aaron/forpic/master/uPic/image-20220209213533055.png" alt="image-20220209213533055" style="zoom:50%;" /></p>

<h3 id="similarity-search">Similarity Search</h3>

<h4 id="cosine-similarity">Cosine Similarity</h4>

<p>With respect to query text: {cheap, quiet, nice, hotel}</p>

<ol>
  <li>Convert query text to query vector [1, 1, 1, 0, 1]</li>
</ol>

<p><img src="https://raw.githubusercontent.com/Dr-Aaron/forpic/master/uPic/image-20220209214129073.png" alt="image-20220209214129073" style="zoom:50%;" /></p>

<ol>
  <li>
    <p>We need a similarity measure between query and documents</p>

    <p>[1, 1, 1, 0, 1]</p>

    <p><img src="https://raw.githubusercontent.com/Dr-Aaron/forpic/master/uPic/image-20220209214228380.png" alt="image-20220209214228380" style="zoom:50%;" /></p>
  </li>
</ol>

<p>Inner Product: ğª â‹… $ğ± = ğ‘¥_1ğ‘_1 + â‹¯ + ğ‘¥_ğ‘›ğ‘_ğ‘› $</p>

<p>The magnitude/length of a n-dimensional vector $x = [x_1, x_2,â€¦,x_n]$
\(||x||=\sqrt[]{x_1^2+...x_n^2}\)
Cosine similarity measures the cosine of the angle between vectors(ä½™å¼¦ç›¸ä¼¼åº¦ï¼šå‘é‡ä¹‹é—´çš„å¤¹è§’ä½™å¼¦)
\(CosSim(q,x)=q\cdot x \over ||q||\ |x||\)
Inner Product and Cosine Similarity</p>

<ul>
  <li>
    <p>Both are defined in the inner product space</p>
  </li>
  <li>
    <p>Cosine similarity only cares about angle difference(ä½™å¼¦ç›¸ä¼¼åº¦åªå…³å¿ƒè§’åº¦)</p>
  </li>
  <li>
    <p>Inner product cares about angle and magnitude(å†…ç§¯å…³æ³¨è§’åº¦å’Œå¤§å°)</p>
  </li>
</ul>

<h4 id="tf-idf">TF-IDF</h4>

<p>TF-IDF: Determine the importance of a Word, Term Frequency.</p>

<ol>
  <li>
    <p>More frequent terms in a document are more important. $tf_{ij} = frequency\  of\  term\ i\ in \ document \ j$</p>
  </li>
  <li>
    <p>Inverse ducument frequency IDF</p>

    <p>Terms that appear in many different documents are less indicative of overall topic in a document.</p>

    <p>$ğ‘‘ğ‘“_ğ‘–$ = number of documents containing term ğ‘–</p>

    <p>$ğ‘–ğ‘‘ğ‘“_ğ‘–$ = inverse document frequency of term ğ‘–</p>

    <p>â€‹        = $log_2(\cfrac N{df_i})$</p>
  </li>
</ol>

<p>â€‹	TF-IDF weighting: The combined term importance indicator is called tf-idf weighting:  $w_{ij}=tf_{ij}\cdot idf_i$</p>

<ul>
  <li>A term has high weight when: <strong>it occurs frequently in the document, but rarely in the rest of the collection.</strong></li>
</ul>

<p>==Exercise slide53==</p>

<p><img src="https://raw.githubusercontent.com/Dr-Aaron/forpic/master/uPic/image-20220210143256766.png" alt="image-20220210143256766" style="zoom: 50%;" /></p>

<p><img src="https://raw.githubusercontent.com/Dr-Aaron/forpic/master/uPic/image-20220210143328410.png" alt="image-20220210143328410" style="zoom: 50%;" /></p>

<h2 id="unsupervised-algorithms">Unsupervised Algorithms</h2>

<h3 id="clustering">Clustering</h3>

<h4 id="applications-and-concepts">Applications and Concepts</h4>

<p>Clustering is the task of grouping a set of objects in such a way that objects in the same group are more similar(<strong>similarity measures</strong>ç›¸ä¼¼åº¦åº¦é‡) to each other than to those in other groups(clusters).å°†å¯¹è±¡è¿›è¡Œåˆ†ç»„ï¼Œä½¿åŒä¸€ç»„å¯¹è±¡ä¸­çš„ç›¸ä¼¼ç¨‹åº¦æ¯”å…¶ä»–ç»„æ›´é«˜</p>

<p>Clustering is used:</p>

<ul>
  <li>As a stand-tool to get insight inito data distribution.(é›†ç¾¤å¯è§†åŒ–)</li>
  <li>As a preprocessing step for other algorithms.(æ•°æ®æ¸…æ´—å’Œå‹ç¼©)</li>
</ul>

<p><strong>Outlier</strong> Analysis by clustering</p>

<ul>
  <li>Outliers are objects that do not belong to any cluster or form clusters of very small cardinality</li>
  <li>Distances on numerical values</li>
</ul>

<p><img src="https://raw.githubusercontent.com/Dr-Aaron/forpic/master/uPic/image-20220210151219703.png" alt="image-20220210151219703" style="zoom: 50%;" /></p>

<p><img src="https://raw.githubusercontent.com/Dr-Aaron/forpic/master/uPic/image-20220210151605931.png" alt="image-20220210151605931" style="zoom:50%;" /></p>

<p>Euclidean distance:æ¬§æ°è·
\(d(x_i, x_j)= \sqrt[2]{|x_{i1}-x_{j1}|^2+|x_{i2}-x_{j2}|+...+|x_{ip}-x_{jp}|^2}\)</p>

<ul>
  <li>$d(x_i, x_j)\ge0$  (non-negativity)</li>
  <li>$d(x_i, x_j)=0$ (coincidence) å¥‘åˆ</li>
  <li>$d(x_i, x_j)=d(x_j, x_i)$ (symmetry) å¯¹ç§°</li>
  <li>$d(x_i, x_j)\le d(x_i, x_k)+d(x_k, x_j)$ (triangular inequality)ä¸‰è§’ä¸ç­‰å¼</li>
</ul>

<p>Also one can use <strong>weighted</strong> distance
\(d(x_i, x_j)= \sqrt[2]{w_1|x_{i1}-x_{j1}|^2+w_2|x_{i2}-x_{j2}|+...+w_p|x_{ip}-x_{jp}|^2}\)
The centroid or geometric center of a plane figure is the arthmetic mean position of all the points in the shape.å¹³é¢å›¾å½¢çš„è´¨å¿ƒ/å‡ ä½•ä¸­å¿ƒæ˜¯è¯¥å½¢çŠ¶æ‰€æœ‰ç‚¹åæ ‡ç®—æœ¯å¹³å‡å€¼ä½ç½®</p>

<p><img src="https://raw.githubusercontent.com/Dr-Aaron/forpic/master/uPic/image-20220210153456475.png" alt="image-20220210153456475" /></p>

<h4 id="k-means">K-Means</h4>

<p>Partitioning methodåˆ’åˆ†æ–¹æ³•ï¼šConstruct a partition of a database D of objects into a set of k clusters.å°†æ•°æ®åº“Dä¸­çš„Nä¸ªå¯¹è±¡åˆ’åˆ†ä¸ºkä¸ªé›†ç¾¤</p>

<ul>
  <li>Each cluster is represented by the center of the cluster.</li>
</ul>

<ol>
  <li>k initial <strong>random centroids</strong> in the data domain. åˆå§‹æ•°æ®åŸŸä¸­çš„éšæœºè´¨å¿ƒ</li>
  <li>Assign objects to nearest centroid to from clusters.</li>
  <li>Update centroids by conputing the mean ofa each cluster.é€šè¿‡è®¡ç®—æ¯ä¸ªç°‡çš„å¹³å‡å€¼è®¡ç®—è´¨å¿ƒ</li>
  <li>go to step 2</li>
</ol>

<p><img src="https://raw.githubusercontent.com/Dr-Aaron/forpic/master/uPic/image-20220210155010107.png" alt="image-20220210155010107" style="zoom:67%;" /></p>

<p>Pseudo-code</p>

<p><img src="https://raw.githubusercontent.com/Dr-Aaron/forpic/master/uPic/image-20220210155208934.png" alt="image-20220210155208934" style="zoom:50%;" /></p>

<p><strong>Strength</strong>: Relatively efficient: O(tkn), wheren is # objects, k is $cluster, and t is #iteration.(å¯¹è±¡æ•°é‡ï¼Œç°‡æ•°é‡ï¼Œ è¿­ä»£æ¬¡æ•°) Normallymk,tÂ«n</p>

<p><strong>Weakness</strong>:</p>

<ul>
  <li>
    <p>Applicable only when <em>mean</em> is defined (what about categorical data)?ä»…åœ¨å¹³å‡æ•°æ®æœ‰æ•ˆæ—¶æœ‰ç”¨</p>
  </li>
  <li>
    <p>Need to specify <em>k,</em> the <em>number</em> of clusters, in advanceéœ€è¦åˆ¶å®šç°‡çš„æ•°é‡</p>
  </li>
  <li>
    <p>Unable to handle noisy data and outliers.æ— æ³•å¤„ç†å™ªå£°å’Œå¼‚å¸¸å€¼</p>
  </li>
  <li>
    <p>Not suitable to discover clusters with non-convex shapes.ä¸èƒ½åˆ¤æ–­éå‡¸ç°‡</p>
  </li>
</ul>

<p>Distance between clusters</p>

<ul>
  <li>Single Link: smallest distance between any points in two clusters</li>
  <li>Complete Link: largest distance between any points in two clusters</li>
  <li>Centroid: distance between the centroids of two clusters</li>
  <li>Average Link: average distance of all pairwise points in clusters</li>
  <li>Average of the distances of the 4*3 pairs of points in the example</li>
</ul>

:ET